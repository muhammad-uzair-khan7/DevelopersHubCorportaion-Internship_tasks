{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Context-Aware RAG Chatbot\n",
    "### Using LangChain + FAISS + Groq + Streamlit\n",
    "\n",
    "| Component | Technology |\n",
    "|-----------|------------|\n",
    "| **Orchestration** | LangChain |\n",
    "| **Embeddings** | `sentence-transformers/all-MiniLM-L6-v2` (local, free) |\n",
    "| **Vector Store** | FAISS (local, fast) |\n",
    "| **LLM** | Groq API â€” Llama 3 (free tier) |\n",
    "| **Memory** | `ConversationBufferWindowMemory` (last 6 turns) |\n",
    "| **UI** | Streamlit |\n",
    "| **Knowledge Base** | 5 custom topic documents (~5K words) |\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "User Query â†’ Embed Query â†’ FAISS Search â†’ Top-K Docs\n",
    "                                              â†“\n",
    "           Conversation Memory â†’ Build Prompt â†’ Groq LLM â†’ Answer + Sources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "â±ï¸ *Takes ~2 minutes on first run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain langchain-community langchain-core faiss-cpu \\\n",
    "             sentence-transformers groq streamlit python-dotenv \\\n",
    "             pyngrok -q\n",
    "print('âœ… All packages installed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ComplexWarning' from 'numpy.core.numeric' (C:\\Users\\Uzair\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\numeric.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m \n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\backend\\__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     _save_pretrained_wrapper,\n\u001b[32m      8\u001b[39m     backend_should_export,\n\u001b[32m      9\u001b[39m     backend_warn_to_save,\n\u001b[32m     10\u001b[39m     save_or_push_to_hub_model,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m __all__ = [\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mload_onnx_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mload_openvino_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msave_or_push_to_hub_model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\backend\\quantize.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Literal\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_or_push_to_hub_model\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[32m      9\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\util\\__init__.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhard_negatives\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mine_hard_negatives\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m append_to_last_row, disable_datasets_caching, disable_logging, fullname, import_from_string\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrieval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     community_detection,\n\u001b[32m     17\u001b[39m     information_retrieval,\n\u001b[32m     18\u001b[39m     paraphrase_mining,\n\u001b[32m     19\u001b[39m     paraphrase_mining_embeddings,\n\u001b[32m     20\u001b[39m     semantic_search,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     cos_sim,\n\u001b[32m     24\u001b[39m     dot_score,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     pytorch_cos_sim,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     _convert_to_batch,\n\u001b[32m     36\u001b[39m     _convert_to_batch_tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     truncate_embeddings,\n\u001b[32m     44\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\util\\retrieval.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cos_sim\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_embeddings\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\util\\similarity.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\__init__.py:83\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     81\u001b[39m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[32m     86\u001b[39m     __all__ = [\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshow_versions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclass_weight\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# mypy error: Module 'numpy.core.numeric' has no attribute 'ComplexWarning'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumeric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ComplexWarning' from 'numpy.core.numeric' (C:\\Users\\Uzair\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\numeric.py)"
     ]
    }
   ],
   "source": [
    "# Verify imports\n",
    "import langchain \n",
    "import faiss \n",
    "import sentence_transformers \n",
    "import groq\n",
    "import streamlit\n",
    "print(f'langchain         : {langchain.__version__}')\n",
    "print(f'sentence-transformers: {sentence_transformers.__version__}')\n",
    "print(f'groq              : {groq.__version__}')\n",
    "print(f'streamlit         : {streamlit.__version__}')\n",
    "print('âœ… All imports verified!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Project Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('knowledge_base', exist_ok=True)\n",
    "print('âœ… Project directories created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile knowledge_base/artificial_intelligence.txt\n",
    "ARTIFICIAL INTELLIGENCE â€” KNOWLEDGE BASE\n",
    "=========================================\n",
    "\n",
    "DEFINITION AND OVERVIEW\n",
    "------------------------\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence processes by computer systems.\n",
    "These processes include learning, reasoning, and self-correction. AI is broadly categorized into\n",
    "Narrow AI (ANI), General AI (AGI), and Superintelligence (ASI).\n",
    "\n",
    "HISTORY OF AI\n",
    "-------------\n",
    "AI as a formal field began in 1956 at the Dartmouth Conference, proposed by John McCarthy and others.\n",
    "The modern AI renaissance began around 2012 with deep learning breakthroughs (AlexNet winning ImageNet).\n",
    "\n",
    "KEY AI SUBFIELDS\n",
    "----------------\n",
    "Machine Learning: Systems that learn from data without explicit programming.\n",
    "Natural Language Processing: Understanding and generating human language.\n",
    "Computer Vision: Interpreting visual information from images and video.\n",
    "Robotics: Physical AI agents that interact with the real world.\n",
    "Expert Systems: AI that emulates decision-making of human experts.\n",
    "\n",
    "MODERN AI ACHIEVEMENTS\n",
    "----------------------\n",
    "Deep Blue defeated chess champion Kasparov in 1997. AlphaGo defeated Go champion Lee Sedol in 2016.\n",
    "GPT-3 (2020) and ChatGPT (2022) demonstrated remarkable language capabilities at scale.\n",
    "GPT-4 (2023) achieved near-human performance on professional exams.\n",
    "AlphaFold solved the protein structure prediction problem (2020).\n",
    "\n",
    "AI ETHICS AND SAFETY\n",
    "--------------------\n",
    "Key concerns: algorithmic bias, privacy, accountability, job displacement, and existential risk.\n",
    "The EU AI Act (2024) is the world's first comprehensive AI regulation framework.\n",
    "Organizations like Anthropic and DeepMind have active AI safety research programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile knowledge_base/machine_learning.txt\n",
    "MACHINE LEARNING â€” KNOWLEDGE BASE\n",
    "==================================\n",
    "\n",
    "DEFINITION\n",
    "----------\n",
    "Machine Learning enables systems to learn from data and improve without being explicitly programmed.\n",
    "Given enough examples, algorithms identify patterns and make decisions with minimal human intervention.\n",
    "\n",
    "THREE MAIN PARADIGMS\n",
    "--------------------\n",
    "1. SUPERVISED LEARNING: Learns from labeled training data (input-output pairs).\n",
    "   Examples: classification (spam detection), regression (house price prediction).\n",
    "   Algorithms: Linear Regression, Decision Trees, Random Forests, SVM, Neural Networks.\n",
    "\n",
    "2. UNSUPERVISED LEARNING: Finds hidden patterns in unlabeled data.\n",
    "   Examples: customer segmentation, anomaly detection, dimensionality reduction.\n",
    "   Algorithms: K-Means, DBSCAN, PCA, Autoencoders, GANs.\n",
    "\n",
    "3. REINFORCEMENT LEARNING: Agent learns by interacting with environment for rewards/penalties.\n",
    "   Examples: game playing (AlphaGo), robotics, recommendation systems.\n",
    "   Algorithms: Q-Learning, DQN, Proximal Policy Optimization (PPO).\n",
    "\n",
    "DEEP LEARNING\n",
    "-------------\n",
    "Deep Learning uses multi-layer neural networks to learn hierarchical representations.\n",
    "CNNs excel at image data. RNNs/LSTMs handle sequential data. Transformers dominate NLP.\n",
    "\n",
    "ML WORKFLOW\n",
    "-----------\n",
    "Problem Definition â†’ Data Collection â†’ EDA â†’ Feature Engineering â†’ Model Selection\n",
    "â†’ Training â†’ Evaluation â†’ Hyperparameter Tuning â†’ Deployment â†’ Monitoring\n",
    "\n",
    "COMMON PROBLEMS\n",
    "---------------\n",
    "Overfitting: Model memorizes training data; fix with regularization, dropout, more data.\n",
    "Underfitting: Model too simple; fix with more complex model or better features.\n",
    "Data Imbalance: One class dominates; fix with SMOTE, class weights.\n",
    "\n",
    "EVALUATION METRICS\n",
    "------------------\n",
    "Classification: Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n",
    "Regression: MAE, MSE, RMSE, R-squared.\n",
    "Frameworks: scikit-learn, PyTorch, TensorFlow, XGBoost, Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile knowledge_base/llm_and_rag.txt\n",
    "LARGE LANGUAGE MODELS & RAG â€” KNOWLEDGE BASE\n",
    "=============================================\n",
    "\n",
    "LARGE LANGUAGE MODELS (LLMs)\n",
    "-----------------------------\n",
    "LLMs are neural networks trained on massive text datasets to understand and generate language.\n",
    "Based on the Transformer architecture (Vaswani et al., 2017). Trained as next-token predictors.\n",
    "Modern LLMs: GPT-4, Claude 3, Gemini, Llama 3, Mistral, Mixtral.\n",
    "\n",
    "TRAINING TECHNIQUES\n",
    "-------------------\n",
    "Pre-training: Self-supervised learning on massive unlabeled text corpora.\n",
    "Fine-tuning: Supervised adaptation on task-specific labeled data.\n",
    "RLHF: Reinforcement Learning from Human Feedback â€” aligns to human preferences.\n",
    "Constitutional AI: Anthropic's method using self-critique and revision.\n",
    "\n",
    "RETRIEVAL-AUGMENTED GENERATION (RAG)\n",
    "--------------------------------------\n",
    "RAG augments LLMs with external knowledge retrieval at inference time.\n",
    "Instead of relying only on parametric memory (weights), RAG retrieves relevant documents\n",
    "and injects them as context into the LLM prompt.\n",
    "\n",
    "RAG PIPELINE STEPS:\n",
    "1. Document Ingestion: Load docs â†’ chunk â†’ embed â†’ store in vector DB\n",
    "2. Retrieval: Embed query â†’ similarity search â†’ return top-K chunks\n",
    "3. Augmentation: Inject retrieved context into LLM prompt\n",
    "4. Generation: LLM produces answer grounded in retrieved facts\n",
    "\n",
    "WHY RAG MATTERS\n",
    "---------------\n",
    "Reduces hallucination by grounding responses in retrieved facts.\n",
    "Enables real-time knowledge without model retraining.\n",
    "Supports source citations â€” users can verify information.\n",
    "Cost-effective alternative to full fine-tuning for domain adaptation.\n",
    "\n",
    "VECTOR DATABASES\n",
    "----------------\n",
    "FAISS: Fast local vector search (Meta, open source) â€” ideal for development.\n",
    "Pinecone: Managed cloud vector DB â€” production ready.\n",
    "Chroma: Simple Python-native, great for prototyping.\n",
    "Weaviate, Qdrant, Milvus: Alternative open-source options.\n",
    "\n",
    "EMBEDDING MODELS\n",
    "----------------\n",
    "all-MiniLM-L6-v2: Fast, 384-dim, excellent for RAG (free, local).\n",
    "text-embedding-ada-002: High quality, OpenAI API-based.\n",
    "BGE-large-en: State-of-the-art open embedding model.\n",
    "\n",
    "RAG EVALUATION (RAGAS)\n",
    "----------------------\n",
    "Context Precision: Are retrieved chunks relevant?\n",
    "Context Recall: Are all relevant chunks retrieved?\n",
    "Faithfulness: Is the answer grounded in the context?\n",
    "Answer Relevance: Does the answer address the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile knowledge_base/prompt_engineering.txt\n",
    "PROMPT ENGINEERING â€” KNOWLEDGE BASE\n",
    "=====================================\n",
    "\n",
    "DEFINITION\n",
    "----------\n",
    "Prompt engineering is designing and optimizing inputs to guide LLMs toward desired outputs.\n",
    "It is a critical skill for effectively using LLMs without modifying model weights.\n",
    "\n",
    "CORE TECHNIQUES\n",
    "---------------\n",
    "ZERO-SHOT: Task instruction only, no examples. Best for simple, well-defined tasks.\n",
    "FEW-SHOT: 2-10 input/output examples before the query. Best for format-specific tasks.\n",
    "CHAIN-OF-THOUGHT (CoT): \"Let's think step by step\" â€” improves multi-step reasoning.\n",
    "ROLE PROMPTING: \"You are an expert X\" â€” activates relevant knowledge associations.\n",
    "STRUCTURED OUTPUT: Request JSON/XML format â€” ensures parseable, consistent responses.\n",
    "\n",
    "PROMPT COMPONENTS\n",
    "-----------------\n",
    "Role/Persona: Who is the AI in this context?\n",
    "Context/Background: Relevant information to inform the response.\n",
    "Task/Instruction: What specifically should the model do?\n",
    "Input Data: The actual content to process.\n",
    "Output Format: How should the response be structured?\n",
    "Constraints: What should/shouldn't be included?\n",
    "Examples: Few-shot demonstrations.\n",
    "\n",
    "ADVANCED TECHNIQUES\n",
    "-------------------\n",
    "ReAct: Interleave Reasoning + Acting (tool calls).\n",
    "Tree of Thoughts: Explore multiple reasoning paths and evaluate intermediates.\n",
    "Self-Consistency: Sample multiple responses and take majority answer.\n",
    "DSPy: Programmatic prompt optimization using automatic compilation.\n",
    "\n",
    "ANTI-PATTERNS TO AVOID\n",
    "----------------------\n",
    "Vague instructions, contradictory constraints, ambiguous pronouns,\n",
    "overloaded prompts, missing output format specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile knowledge_base/python_programming.txt\n",
    "PYTHON PROGRAMMING â€” KNOWLEDGE BASE\n",
    "=====================================\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Python is a high-level, interpreted, general-purpose programming language created by Guido van Rossum (1991).\n",
    "It emphasizes readability and supports procedural, OOP, and functional paradigms.\n",
    "Most popular language globally; dominates data science, ML, and AI development.\n",
    "\n",
    "KEY FEATURES\n",
    "------------\n",
    "Dynamic typing, garbage collection, first-class functions, list/dict comprehensions,\n",
    "context managers (with), decorators, generators, and optional type hints (PEP 484).\n",
    "\n",
    "DATA STRUCTURES\n",
    "---------------\n",
    "List: Ordered, mutable. | Tuple: Ordered, immutable. | Dict: Key-value mapping.\n",
    "Set: Unique elements. | Deque: Double-ended queue, O(1) appends.\n",
    "\n",
    "DATA SCIENCE ECOSYSTEM\n",
    "----------------------\n",
    "NumPy: N-dimensional arrays and numerical computing.\n",
    "Pandas: DataFrames for data manipulation and analysis.\n",
    "Matplotlib/Seaborn: Visualization libraries.\n",
    "Scikit-learn: Machine learning algorithms.\n",
    "PyTorch/TensorFlow: Deep learning frameworks.\n",
    "\n",
    "BEST PRACTICES\n",
    "--------------\n",
    "Follow PEP 8 style guide. Use virtual environments (venv/conda).\n",
    "Write docstrings. Use type hints. Write pytest unit tests.\n",
    "Use f-strings (Python 3.6+). Prefer pathlib over os.path.\n",
    "\n",
    "WEB FRAMEWORKS\n",
    "--------------\n",
    "FastAPI: Modern async REST API framework â€” fastest Python web framework.\n",
    "Django: Full-stack web framework with batteries included.\n",
    "Flask: Lightweight microframework for simple web apps.\n",
    "Streamlit: Rapid data app development â€” ideal for ML demos.\n",
    "Gradio: Simple ML model interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the RAG Pipeline Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_pipeline.py\n",
    "\"\"\"\n",
    "RAG Pipeline: LangChain + FAISS + HuggingFace Embeddings + Groq LLM\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from groq import Groq\n",
    "\n",
    "KB_DIR = Path(\"knowledge_base\")\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    SYSTEM_PROMPT = \"\"\"You are a knowledgeable AI assistant with access to a curated knowledge base.\n",
    "Use the retrieved context to answer questions accurately. Always cite which document your answer\n",
    "comes from. If context is insufficient, say so and answer from general knowledge.\"\"\"\n",
    "\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "[CONVERSATION HISTORY]\n",
    "{history}\n",
    "\n",
    "[RETRIEVED CONTEXT]\n",
    "{context}\n",
    "\n",
    "[CURRENT QUESTION]\n",
    "{question}\n",
    "\n",
    "Answer using the retrieved context and conversation history. Be specific and cite sources:\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.groq_client = Groq(api_key=api_key)\n",
    "        self.memory = ConversationBufferWindowMemory(\n",
    "            k=6, return_messages=False,\n",
    "            human_prefix=\"Human\", ai_prefix=\"Assistant\"\n",
    "        )\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"context\", \"question\"],\n",
    "            template=self.PROMPT_TEMPLATE\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "        self._build_vectorstore()\n",
    "\n",
    "    def _load_docs(self) -> List[Document]:\n",
    "        docs = []\n",
    "        for fp in KB_DIR.glob(\"*.txt\"):\n",
    "            text = fp.read_text(encoding=\"utf-8\")\n",
    "            docs.append(Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": fp.stem.replace(\"_\", \" \").title(), \"file\": fp.name}\n",
    "            ))\n",
    "            print(f\"  âœ“ Loaded: {fp.name} ({len(text)} chars)\")\n",
    "        return docs\n",
    "\n",
    "    def _build_vectorstore(self):\n",
    "        print(\"ğŸ“š Loading knowledge base...\")\n",
    "        docs = self._load_docs()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=60)\n",
    "        chunks = splitter.split_documents(docs)\n",
    "        print(f\"âœ‚ï¸  {len(chunks)} chunks created\")\n",
    "\n",
    "        print(\"ğŸ”¢ Generating embeddings...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "        self.vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        print(\"âœ… Vector store ready!\")\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Document]:\n",
    "        return self.vectorstore.similarity_search(query, k=top_k)\n",
    "\n",
    "    def chat(self, question: str, top_k: int = 3,\n",
    "             model: str = \"llama3-8b-8192\", **kwargs) -> Dict[str, Any]:\n",
    "        # Retrieve\n",
    "        docs = self.retrieve(question, top_k=top_k)\n",
    "\n",
    "        # Format context\n",
    "        context_parts, sources = [], []\n",
    "        seen = set()\n",
    "        for doc in docs:\n",
    "            src = doc.metadata.get(\"source\", \"Unknown\")\n",
    "            context_parts.append(f\"[{src}]\\n{doc.page_content}\")\n",
    "            if src not in seen:\n",
    "                sources.append(src)\n",
    "                seen.add(src)\n",
    "        context_str = \"\\n\\n---\\n\\n\".join(context_parts) or \"No context available.\"\n",
    "\n",
    "        # Get history\n",
    "        history = self.memory.load_memory_variables({}).get(\"history\", \"No prior conversation.\")\n",
    "\n",
    "        # Build prompt & call LLM\n",
    "        filled = self.prompt.format(history=history, context=context_str, question=question)\n",
    "        response = self.groq_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\",   \"content\": filled},\n",
    "            ],\n",
    "            temperature=0.4,\n",
    "            max_tokens=800,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Update memory\n",
    "        self.memory.save_context({\"input\": question}, {\"output\": answer})\n",
    "\n",
    "        return {\"answer\": answer, \"sources\": sources, \"contexts\": docs}\n",
    "\n",
    "    def clear_memory(self): self.memory.clear()\n",
    "\n",
    "\n",
    "def build_pipeline(api_key: str) -> RAGPipeline:\n",
    "    return RAGPipeline(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test RAG Pipeline in Notebook (No UI)\n",
    "\n",
    "Run this cell to verify the pipeline works before launching Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Enter your Groq API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Free key: https://console.groq.com  (sign up â†’ API Keys â†’ Create key)\n",
    "GROQ_API_KEY = \"gsk_your_key_here\"   # â† REPLACE WITH YOUR KEY\n",
    "\n",
    "# Build pipeline\n",
    "from rag_pipeline import build_pipeline\n",
    "pipeline = build_pipeline(GROQ_API_KEY)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… RAG Pipeline Ready! Running test queries...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 1: RAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "q1 = \"What is Retrieval-Augmented Generation and why is it important?\"\n",
    "result1 = pipeline.chat(q1, top_k=3)\n",
    "\n",
    "print(f\"â“ Question : {q1}\")\n",
    "print(f\"ğŸ“š Sources  : {', '.join(result1['sources'])}\")\n",
    "print(f\"ğŸ¤– Answer   :\\n{result1['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 2: Follow-up (tests MEMORY) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "q2 = \"What vector databases would you recommend for it?\"\n",
    "result2 = pipeline.chat(q2, top_k=3)\n",
    "\n",
    "print(f\"â“ Question : {q2}\")\n",
    "print(f\"ğŸ“š Sources  : {', '.join(result2['sources'])}\")\n",
    "print(f\"ğŸ¤– Answer   :\\n{result2['answer']}\")\n",
    "print(\"\\nğŸ’¡ Note: The answer references the previous question context (memory working!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test Query 3: Different topic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "q3 = \"Explain the difference between supervised and unsupervised learning with examples.\"\n",
    "result3 = pipeline.chat(q3, top_k=3)\n",
    "\n",
    "print(f\"â“ Question : {q3}\")\n",
    "print(f\"ğŸ“š Sources  : {', '.join(result3['sources'])}\")\n",
    "print(f\"ğŸ¤– Answer   :\\n{result3['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Show Conversation Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "history = pipeline.memory.load_memory_variables({})[\"history\"]\n",
    "print(\"ğŸ“– CONVERSATION HISTORY (stored in memory):\")\n",
    "print(\"=\" * 60)\n",
    "print(history[:1500] + \"...\" if len(history) > 1500 else history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Retrieval Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Show similarity scores for a test query\n",
    "test_query = \"How do vector databases work for semantic search?\"\n",
    "retrieved = pipeline.vectorstore.similarity_search_with_score(test_query, k=5)\n",
    "\n",
    "labels = [f\"{doc.metadata['source']}\\n({doc.page_content[:30]}...)\" for doc, _ in retrieved]\n",
    "scores = [1 - score for _, score in retrieved]  # Convert distance to similarity\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores)))\n",
    "bars = ax.barh(range(len(labels)), scores, color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_yticklabels(labels, fontsize=9)\n",
    "ax.set_xlabel('Cosine Similarity Score')\n",
    "ax.set_title(f'Retrieved Documents for:\\n\"{test_query}\"', fontweight='bold')\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{score:.3f}', va='center', fontsize=10)\n",
    "ax.set_xlim(0, 1.1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('retrieval_scores.png', dpi=130, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('âœ… Retrieval visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Streamlit App File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"Context-Aware RAG Chatbot â€” Streamlit UI\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Chatbot\", page_icon=\"ğŸ¤–\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    ".user-msg { background: #1e3a5f; color: white; padding: 12px 16px;\n",
    "           border-radius: 15px 15px 3px 15px; margin: 5px 0; max-width: 75%; margin-left: auto; }\n",
    ".bot-msg  { background: #1a1a2e; color: #e8e8f0; padding: 12px 16px;\n",
    "           border-radius: 15px 15px 15px 3px; margin: 5px 0; max-width: 80%;\n",
    "           border: 1px solid #3a3a6a; }\n",
    ".src-badge { display:inline-block; background: rgba(100,120,255,0.2); color: #8888ff;\n",
    "             border: 1px solid #5555cc; border-radius: 10px; padding: 2px 8px;\n",
    "             font-size: 11px; margin: 2px; }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_pipeline(api_key):\n",
    "    from rag_pipeline import build_pipeline\n",
    "    return build_pipeline(api_key)\n",
    "\n",
    "# Session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with st.sidebar:\n",
    "    st.title(\"ğŸ¤– RAG Chatbot\")\n",
    "    st.caption(\"LangChain Â· FAISS Â· Groq Â· Context Memory\")\n",
    "    st.divider()\n",
    "\n",
    "    api_key = st.text_input(\"ğŸ”‘ Groq API Key\", type=\"password\",\n",
    "                            placeholder=\"gsk_...\", value=os.getenv(\"GROQ_API_KEY\", \"\"))\n",
    "    model = st.selectbox(\"ğŸ§  Model\",\n",
    "        [\"llama3-8b-8192\", \"llama3-70b-8192\", \"mixtral-8x7b-32768\", \"gemma2-9b-it\"])\n",
    "    top_k = st.slider(\"ğŸ“š Top-K Docs\", 1, 6, 3)\n",
    "    show_src = st.toggle(\"ğŸ” Show Sources\", True)\n",
    "\n",
    "    if st.button(\"ğŸš€ Initialize\", use_container_width=True, type=\"primary\"):\n",
    "        if not api_key:\n",
    "            st.error(\"Enter your API key first.\")\n",
    "        else:\n",
    "            with st.spinner(\"Building RAG pipeline (~30s on first run)...\"):\n",
    "                try:\n",
    "                    p = load_pipeline(api_key)\n",
    "                    st.session_state.pipeline = p\n",
    "                    st.session_state.ready = True\n",
    "                    st.success(\"âœ… Ready!\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error: {e}\")\n",
    "\n",
    "    st.divider()\n",
    "    st.markdown(\"**ğŸ“š Knowledge Base:**\")\n",
    "    st.markdown(\"- ğŸ¤– Artificial Intelligence\\n- ğŸ§¬ Machine Learning\\n- ğŸŒ LLMs & RAG\\n- ğŸ’¡ Prompt Engineering\\n- ğŸ Python\")\n",
    "\n",
    "    if st.button(\"ğŸ—‘ï¸ Clear Chat\", use_container_width=True):\n",
    "        st.session_state.messages = []\n",
    "        if hasattr(st.session_state, 'pipeline'):\n",
    "            st.session_state.pipeline.clear_memory()\n",
    "        st.rerun()\n",
    "\n",
    "    if st.session_state.get('ready'):\n",
    "        st.metric(\"ğŸ’¬ Turns\", len(st.session_state.messages) // 2)\n",
    "\n",
    "# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "st.title(\"ğŸ’¬ Context-Aware RAG Chatbot\")\n",
    "st.caption(\"Retrieval-Augmented Generation with Conversational Memory\")\n",
    "\n",
    "if not st.session_state.get('ready'):\n",
    "    st.info(\"ğŸ‘ˆ Enter your Groq API key in the sidebar and click **Initialize**.\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### ğŸ’¡ Try asking:\n",
    "    - *\"What is RAG and how does it work?\"*\n",
    "    - *\"Explain the transformer architecture\"*\n",
    "    - *\"What's the difference between supervised and unsupervised learning?\"*\n",
    "    - *\"How do vector databases enable semantic search?\"*\n",
    "    - *\"What are the best practices for prompt engineering?\"*\n",
    "    \"\"\")\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f'<div class=\"user-msg\">ğŸ‘¤ {msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.markdown(f'<div class=\"bot-msg\">ğŸ¤– {msg[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "        if show_src and msg.get(\"sources\"):\n",
    "            badges = \" \".join(f'<span class=\"src-badge\">ğŸ“„ {s}</span>' for s in msg[\"sources\"])\n",
    "            st.markdown(f\"Sources: {badges}\", unsafe_allow_html=True)\n",
    "\n",
    "if prompt := st.chat_input(\"Ask a question...\", disabled=not st.session_state.get('ready')):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.spinner(\"ğŸ” Retrieving & generating...\"):\n",
    "        try:\n",
    "            res = st.session_state.pipeline.chat(prompt, top_k=top_k, model=model)\n",
    "            st.session_state.messages.append({\n",
    "                \"role\": \"assistant\", \"content\": res[\"answer\"], \"sources\": res[\"sources\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            st.session_state.messages.append({\"role\": \"assistant\",\n",
    "                \"content\": f\"Error: {e}\", \"sources\": []})\n",
    "    st.rerun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Launch Streamlit in Colab via ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Method 1: Using localtunnel (no account needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, threading, time\n",
    "\n",
    "# Start Streamlit\n",
    "proc = subprocess.Popen(\n",
    "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"],\n",
    "    stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    ")\n",
    "time.sleep(5)\n",
    "\n",
    "# Use colab's port forwarding\n",
    "try:\n",
    "    from google.colab.output import eval_js\n",
    "    url = eval_js(\"google.colab.kernel.proxyPort(8501)\")\n",
    "    print(f\"\\nğŸŒ Streamlit App URL: {url}\")\n",
    "    print(\"  â†’ Click the URL above to open the chatbot!\")\n",
    "except:\n",
    "    print(\"ğŸŒ Streamlit running on port 8501\")\n",
    "    print(\"   If using locally: http://localhost:8501\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: Enter your Groq API key in the sidebar to start chatting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Method 2: Using ngrok (optional, needs free account) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Sign up free at https://ngrok.com\n",
    "# 2. Copy your authtoken from https://dashboard.ngrok.com/auth\n",
    "\n",
    "# NGROK_TOKEN = \"your_ngrok_token_here\"   # â† REPLACE\n",
    "\n",
    "# from pyngrok import ngrok\n",
    "# ngrok.set_auth_token(NGROK_TOKEN)\n",
    "# public_url = ngrok.connect(8501)\n",
    "# print(f\"ğŸŒ Public URL: {public_url}\")\n",
    "# print(\"Share this link with anyone to demo your chatbot!\")\n",
    "\n",
    "print('â„¹ï¸  ngrok method commented out â€” uncomment to get a public shareable URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 7: RAG System Analysis\n",
    "\n",
    "Evaluate the system's retrieval performance and visualize key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Retrieval Quality Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "# Test queries covering different topics\n",
    "test_queries = [\n",
    "    (\"What is RAG and how does FAISS work?\",          \"Llm And Rag\"),\n",
    "    (\"Explain overfitting and how to fix it\",          \"Machine Learning\"),\n",
    "    (\"What is few-shot prompting?\",                    \"Prompt Engineering\"),\n",
    "    (\"History of artificial intelligence\",            \"Artificial Intelligence\"),\n",
    "    (\"Python best practices and virtual environments\", \"Python Programming\"),\n",
    "]\n",
    "\n",
    "# Check if top-1 retrieval matches expected source\n",
    "results = []\n",
    "for query, expected_src in test_queries:\n",
    "    retrieved = pipeline.retrieve(query, top_k=3)\n",
    "    sources = [d.metadata['source'] for d in retrieved]\n",
    "    hit = expected_src in sources\n",
    "    rank = sources.index(expected_src) + 1 if hit else None\n",
    "    results.append({\"query\": query[:45]+\"...\", \"expected\": expected_src,\n",
    "                    \"got\": sources[:3], \"hit\": hit, \"rank\": rank})\n",
    "\n",
    "# Compute MRR (Mean Reciprocal Rank)\n",
    "mrr = np.mean([1/r['rank'] if r['rank'] else 0 for r in results])\n",
    "recall_at_3 = np.mean([r['hit'] for r in results])\n",
    "\n",
    "print(\"ğŸ“Š RETRIEVAL EVALUATION\")\n",
    "print(\"=\" * 65)\n",
    "for r in results:\n",
    "    status = f\"âœ… Rank {r['rank']}\" if r['hit'] else \"âŒ Not found\"\n",
    "    print(f\"  {status} | {r['query']}\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"  Recall@3        : {recall_at_3:.0%}\")\n",
    "print(f\"  Mean Reciprocal Rank (MRR): {mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Architecture Diagram â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('#0d0d1a')\n",
    "fig.patch.set_facecolor('#0d0d1a')\n",
    "ax.set_title('RAG Chatbot Architecture', color='white', fontsize=16, fontweight='bold', pad=10)\n",
    "\n",
    "# Define boxes\n",
    "components = [\n",
    "    (0.3, 2.5, 1.8, 1.2, '#1e3a5f', 'ğŸ‘¤ User\\nQuery',         'white'),\n",
    "    (2.5, 2.5, 1.8, 1.2, '#2d1b4e', 'ğŸ”¢ Embed\\nQuery',       '#c8a0ff'),\n",
    "    (4.7, 2.5, 1.8, 1.2, '#1a3a2a', 'ğŸ” FAISS\\nSearch',      '#80ffaa'),\n",
    "    (6.9, 3.8, 1.8, 1.2, '#3a2a00', 'ğŸ“š Knowledge\\nBase',    '#ffd080'),\n",
    "    (6.9, 1.2, 1.8, 1.2, '#2a1a0a', 'ğŸ§  Memory\\n(k=6 turns)','#ffb060'),\n",
    "    (9.1, 2.5, 1.8, 1.2, '#1a1a3a', 'ğŸ“ Prompt\\nTemplate',   '#a0a8ff'),\n",
    "    (11.3, 2.5, 1.8, 1.2,'#3a1a1a', 'ğŸ¤– Groq LLM\\n(Llama 3)','#ff8080'),\n",
    "]\n",
    "\n",
    "boxes = {}\n",
    "for x, y, w, h, color, label, tc in components:\n",
    "    rect = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.1\",\n",
    "                                    facecolor=color, edgecolor='#4444aa', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center',\n",
    "            color=tc, fontsize=9, fontweight='bold')\n",
    "\n",
    "# Arrows\n",
    "arrows = [\n",
    "    (2.1, 3.1, 0.35, 0),   (4.3, 3.1, 0.35, 0),\n",
    "    (6.5, 3.1, 0.35, 0.7), (6.5, 3.1, 0.35, -0.7),\n",
    "    (8.7, 3.1, 0.35, 0),   (10.9, 3.1, 0.35, 0),\n",
    "]\n",
    "for x, y, dx, dy in arrows:\n",
    "    ax.annotate('', xy=(x+dx, y+dy), xytext=(x, y),\n",
    "                arrowprops=dict(arrowstyle='->', color='#6666cc', lw=1.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('architecture.png', dpi=130, bbox_inches='tight', facecolor='#0d0d1a')\n",
    "plt.show()\n",
    "print('âœ… Architecture diagram saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "| Component | Implementation |\n",
    "|-----------|----------------|\n",
    "| **RAG Framework** | LangChain (chains, memory, prompts) |\n",
    "| **Embeddings** | `all-MiniLM-L6-v2` â€” 384-dim, free, local |\n",
    "| **Vector Store** | FAISS â€” cosine similarity, no server needed |\n",
    "| **Chunking** | `RecursiveCharacterTextSplitter` (500 tokens, 60 overlap) |\n",
    "| **LLM** | Groq Llama 3 8B â€” fast free inference |\n",
    "| **Memory** | `ConversationBufferWindowMemory` â€” last 6 turns |\n",
    "| **UI** | Streamlit â€” dark theme, source citations, session stats |\n",
    "| **Knowledge Base** | 5 topics Ã— ~800 words = ~4000 words corpus |\n",
    "\n",
    "**Skills Demonstrated:** RAG Â· Document Embedding Â· Vector Search Â· LangChain Â· Prompt Engineering Â· Streamlit Deployment Â· Conversational Memory"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task6_RAG_Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
